# [五条0126] Figma+Claude，你的设计系统维护神器
发布日期：2026/01/24

![](https://miro.medium.com/v2/resize:fit:1050/1*1P_sz1na2-EZ4mXU9hZdfg.png)

Design systems break not because designers don’t care but because _maintaining consistency at scale is genuinely hard_. Tokens drift, naming becomes messy, documentation gets outdated, and what started as a neat system slowly turns into a fragile one. But AI can help you keep your design system up to date.

One of my favorite coding tools, Claude, can act as a design systems assistant for another my favorite design tool, Figma, helping with the unglamorous work: create ready-to-use token structures, extracting styles, auditing variables, fixing inconsistencies, and even documenting how the system should be used.

In this article, I’ll show 4 practical workflows where combining Figma & Claude becomes a powerful accelerator for design system work.

## 1\. Generate tokens formatted for **Figma Variables**

_When to use:_ Early in the design process, when you need a robust foundation for style variables on which you can build a consistent, scalable UI.

Claude can help you generate an entire collection of tokens for your design system, including colors, typography, spacing, corner radius, etc. The great thing is that Claude optimizes the output for Figma formatting, so you can paste the output directly into your system.

### Prompt

_Quick note about the prompt:_ I’m using **RTDO** (Role, Task, Details, Output) formatting for my design system prompts, and, for simplicity, I use a general prompt that doesn’t include specific constraints (such as the brand colors your product uses).

You are a design systems expert working inside Figma.  
  
Create a complete token system for Figma Variables with   
these collections:   
– Color   
– Typography   
– Spacing   
– Radius   
– Elevation   
– Motion  
  
Product context:   
– Product type: B2B SaaS dashboard   
– Brand traits: calm, analytical, trustworthy  
  
Output format:   
– Group by Variable Collection   
– Use Figma-style naming (e.g. color.background.primary, space.200, radius.sm) – Show type (color / number / string)   
– Include values   
– Include Light and Dark modes where relevant  

### Output generated by Claude

Below is a sample of output that Claude generated for colors: border colors for dark and light mode.

![](https://miro.medium.com/v2/resize:fit:1050/1*5hkee_2fgdOyIkNWq65V0g.png)

Semantic colors generated by Claude.

## 2\. Convert visual style into Figma-ready tokens

_When to use:_ When you have a hi-fi design in Figma and want to extract style variables from it.

I have this UI design of a dashboard and will feed it to Claude so it can extract variables from it.

![](https://miro.medium.com/v2/resize:fit:1500/1*kuZs99c8qxJJCcQDVjNn1A.png)

Original UI design in Figma.

I will export this UI design as an image, attach it to Claude and write a prompt:

![](https://miro.medium.com/v2/resize:fit:1050/1*Sat_seP17oVfiQtb2gf6eA.png)

### **Prompt**

Translate this visual style into a Figma Variables system.  
  
Create:   
– Color variables   
– Spacing scale   
– Radius scale   
– Typography variables  
  
Use Figma naming conventions and organize by collection.  

### Output generated by Claude

The output that Claude generated for me is not plain text, but an interactive page that outlines all style attributes of my design. I can click _Copy JSON_ and have a collection of colors used in my design in JSON format.

![](https://miro.medium.com/v2/resize:fit:1500/1*Du1aPNqIKeC8YNJ6GJXuyA.png)

Visual style collection. Generated by Claude.

## 3\. Audit style guide

_When to use:_ When you want to quickly identify & fix issues in your styleguide.

To export a collection of Variables from Figma, I will use the [CSS variables import/export](https://www.figma.com/community/plugin/1470777269812001046/css-variables-import-export) plugin. Once you launch the plugin, it will show you what variables you have in your local collection, and you can convert them to CSS that you can later upload to Claude for audit.

![](https://miro.medium.com/v2/resize:fit:1500/1*2om0L21C2T97EIzqgiYx4g.png)

Generating a collection of variables for design system.

Upload this CSS file to Claude and prompt it:

![](https://miro.medium.com/v2/resize:fit:1050/1*YwTSnw2ZCUaAmXH8qwh_Pg.png)

### **Prompt**

I've pasted my Figma Variables as CSS  
  
Audit this system for:  
– Inconsistent naming  
– Redundant variables  
– Missing semantic layers  
– Poor scalability  
– Accessibility risks  
  
Then propose a cleaner, scalable structure still compatible  
with Figma Variables.  

### Output generate by Claude

Claude produces a document that outlines all the problems with our style variables.

![](https://miro.medium.com/v2/resize:fit:1050/1*z5VY3dwvEbJdcfoOdY_nnQ.png)

Design audit for style variables.

But this document alone won’t be very valuable as you still need to fix the actual issues. What makes Claude great is that you can ask it to do the job and fix the original CSS file in a follow-up prompt.

Follow your advice and fix original CSS file.  
  
Don’t add notes/comments to the CSS file; just fix the actual issues.  
  
Provide comments in the chat output only.   

### Output generated by Claude

Claude generated a new CSS file for me:

![](https://miro.medium.com/v2/resize:fit:1050/1*F4Y7bzjIxiLu5PBuUD0HLQ.png)

Extract of the CSS file that Claude generated for me.

And it also outlined a summary of changes in the chat window.

![](https://miro.medium.com/v2/resize:fit:1050/1*YpJkIq1WPnMcTbJA6pkfqA.png)

Extract of changes introduced by Claude.

What I can do next, is to download the CSS file, jump back to Figma and in the CSS variables import/export navigate to _Import from CSS_ tab. Upload new file and click _Create Variables_.

![](https://miro.medium.com/v2/resize:fit:1050/1*RbwyTsldrxx8O_W_tmGpiQ.png)

Create variables in Figma.

And if your file is formatted correctly, you will see this message:

![](https://miro.medium.com/v2/resize:fit:1050/1*7QLuDGKlELmkOLBmPaXkqw.png)

Quick note: Sometimes Claude misses original comments in new CSS file. So if you see an error message like “_Missing required comments. CSS must include “/\* Collection name: \*/” and “/\* Mode: \*/” comments_” in the CSS plugin, simply prompt Claude to

Add required comments.   
  
CSS must include "/* Collection name: */" and "/* Mode: */" comments  
  
similar to original CSS file I've provided  

## 4\. Documenting your style variables

_When to use:_ When you have a final, polished version of your style guide and want to document how to use styles properly in design.

If you successfully completed the previous case, your next logical step is documenting the variables. The good news is that all you need to do is write a follow-up prompt for Claude in the same chat.

### Prompt

Using this Figma token system you've created  
  
Create short internal documentation for designers explaining:   
– How to choose the right color tokens   
– When to use semantic vs base   
– How to use spacing tokens with Auto Layout   
– Common mistakes to avoid  
  
Write this as if it will live in a Figma page next to the library  

Claude will likely generate you a dedicated page in this (or a very similar style).

![](https://miro.medium.com/v2/resize:fit:1050/1*7CCwIgBPrUefauXeoUDOJQ.png)

Documentation for Figma variables. Generated by Claude.

If we want to use this document in our internal resources, we likely need to change the format of this document. So I will provide a follow-up prompt, asking Claude to provide it as a markdown (.md) file.

![](https://miro.medium.com/v2/resize:fit:1500/1*78QzUZKksILPuE2fJ87cVQ.png)

Documentation in markdown format. Generated by Claude.

## Want to master vibe coding?

Join my workshop on January 29, “[Vibe coding in practice](https://maven.com/p/9944eb/vibe-coding-in-practice)” a 30-minute practical session where I’ll show how to build a complex product with Anthropic Claude & Cursor. You will learn how to evaluate AI outputs, make better design decisions, and stay in control of quality produced by AI

[Vibe Coding in Practice](https://maven.com/p/9944eb/vibe-coding-in-practice?source=post%5Fpage-----58c00eb82d90---------------------------------------)

hidden text to trigger resize events if fonts change

---

# [五条0120] AI 不是替代品，而是设计加速器
发布日期：2026/01/20

As a design engineer, a role that's all about craft, thoughtfulness and creativity, I should be skeptical of AI. By definition AI is not particularly great at those things, at least for now. But despite that, I kinda enjoy using it?

When I work on something, whether it's at [Interfere](https://interfere.com/) or my personal projects, I like to experiment a lot. Design engineering is a lot about trial and error, and I often spend hours trying to find the "this feels right" moment.

This is where AI helps. Instead of spending hours on a concept that I'm unsure of, I try that concept out in a matter of minutes, and throw it away if it doesn't feel right.

There is an important distinction to make here though. I don't use AI to come up with ideas or to replace my own thinking. _I use it to accelerate my workflow._ 

It's also very important to understand what the agent is doing. AI works best if the user is in charge and not the other way around. Don't just blindly follow whatever the agent gives you.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Fworkflow-example-2.avif&w=3840&q=75)

With that being said, here's how I use AI everyday as a design engineer.

## Setup

Whether you are using [Claude Code](https://claude.com/product/claude-code), [Codex](https://openai.com/codex/), [Cursor](https://cursor.com/) or any other agent, one of the most important things is the setup. I personally use Cursor for most of my work, so I'll use that as an example.

The first thing I do when starting a project is create a set of rules for the codebase.

You already have a pretty good idea of how you want to do certain things. Whether it's about using , using the utility instead of template literals, design system usage or something else.`will-change` `cn`

I like to keep rules short and to the point, often referencing other files too. This is something that is recommended by Cursor in their [blog](https://cursor.com/blog/agent-best-practices).

Copying someone else's rules and using them without understanding what they actually do isn't all that great. If you're going to copy rules I'd recommend trying understand what they actually do first.

Humans can remember a lot of context across many different conversations. When talking to AI we sometimes assume it thinks the same way, but it doesn't. That's why when I start a conversation with an agent, _I always assume it has no context_, and that tends to lead to the best results.

Depending on the project, I use different sets of rules. For this website, for example, I have a couple which are about animation performance, react compiler, using the util instead of template literals and the design system.`cn`

All of these save me a great amount of time, since I don't have to describe these things every time I talk to an agent and I can always reference them in my prompts.

In addition to my own rules, [ui-skills](https://www.ui-skills.com/) and [Vercel Web Interface Guidelines](https://x.com/JohnPhamous/status/2010777566085595357) are great too. [Motion](https://motion.dev) also has a great set of rules that come with the [plus](https://motion.dev/plus) subscription and [TailwindCSS](https://tailwindcss.com/sponsor) also provides a set of rules when you become a sponsor.

## Commands

In addition to rules, I also use a couple of commands. The first one is , which [Eric](https://x.com/ericzakariasson) from Cursor [shared on X](https://x.com/ericzakariasson/status/1995671800643297542). Depending on the codebase, I also create custom ones.`/deslop`

For example, at [Interfere](https://interfere.com/) we have a custom command that compares the PR against specific rules that we follow in the codebase.`/review`

## Models

I'm not an expert on models by any means, but I've tried a bunch of them, and at the time of writing this article, I use [Claude Opus 4.5](https://www.anthropic.com/news/claude-opus-4-5) as my daily driver. It is pretty fast and for the majority of the things that I use it for, it does a great job.

## Figma MCP

When it comes to building UI, the [Figma MCP](https://help.figma.com/hc/en-us/articles/32132100833559-Guide-to-the-Figma-MCP-server) has been a game changer for me. At [Interfere](https://interfere.com/) we're currently at a point where a lot of UI needs to be built.

[](https://help.figma.com/hc/en-us/articles/32132100833559-Guide-to-the-Figma-MCP-server)

[Figma.logoCreated using Figma](https://help.figma.com/hc/en-us/articles/32132100833559-Guide-to-the-Figma-MCP-server)

I've always been hesitant to use AI for building out UI since I wanted to have full control over every little detail and I thought it would always take longer to build out UI with AI and then polish it, instead of doing it myself.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Ffigma-mcp.avif&w=2048&q=75)

Figma MCP

However, I tried it with Opus 4.5 for the first time a couple weeks ago and I was pleasantly surprised. If your rules are set up well, the output is surprisingly good.

Don't get me wrong, it's not perfect by any means, but building out the scaffolding for complex screens with a lot of components is way faster.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Ffigma-mcp-prompt-example.avif&w=3840&q=75)

You can see that the prompt I used is not very technical. This has generally worked pretty well for me when it comes to building out UI scaffolding, partially because the codebase rules are set up well.

Once the scaffolding is built, I start tweaking, polishing and animating.

I can't stress enough how important it is to actually understand what the agent is doing. If you don't understand, ask the agent to explain (Cursor's Ask mode is great for this).

## Agent Modes & Prompting

Cursor also offers different modes. They are great, I use all of them, but the one I use the most outside of the Agent one is the Plan one.

If I work on a bigger feature, the difference between using the plan mode and not using it is pretty significant.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Fmodes-2.avif&w=3840&q=75)

Another thing that helps a lot is structuring prompts. Structuring your prompt into multiple smaller ones makes the output much better and is also way easier to follow.

## MCPs

Outside of the Figma MCP, there is only one more that I use on a daily basis and it is [context7](https://github.com/upstash/context7). It fetches the most up-to-date documentation for the libraries and frameworks I work with.

These can often be outdated in the LLM training data, so it's pretty useful.

In terms of using the CLI, I tried [Claude Code](https://claude.com/product/claude-code), [Codex](https://openai.com/codex/), [Cursor CLI](https://cursor.com/cli) and [OpenCode](https://opencode.ai/).

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Fopencode.avif%3Fv%3D2&w=3840&q=75)

They are all pretty good, with [Claude Code](https://claude.com/product/claude-code) being my favorite. I mix and match between using an IDE and a CLI, but I've been using the CLI more and more lately.

I find CLIs to be much better at bigger tasks than IDEs or their web counterparts, so I mostly use them for that.

## Code Review

My code review setup is pretty basic. I have Codex set up to review any pull request I make. This way I also leverage my OpenAI subscription. I also use the [Vercel agent](https://vercel.com/docs/agent), but I mostly use it for bigger PRs.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Freview.avif&w=3840&q=75)

Not crucial by any means, but having another layer of code review is always nice.

## Writing Less Code

Another part of my workflow where I leverage AI a lot is writing less code. Not in the sense that AI writes the code for me, therefore I write less code, but where less code is written to achieve the same thing.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Fless-code.avif&w=3840&q=75)

Sometimes you write the same code multiple times across the codebase and it's difficult to keep track of all of these duplications. AI is great at finding them and extracting them into shared components/utilities.

For example, last week I learnt that the icon set ([central icons](https://centralicons.com/)) that I use on this page has an [npm](https://www.npmjs.com/package/@central-icons-react/all) package. Up until now I've been defining icons manually, but now I wanted to migrate.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fai-workflow%2Fcentral-icons-package.avif&w=3840&q=75)

This is a pretty tedious task to do manually, because you have to replace imports, delete files and I didn't really have time to do it. I figured why not ask Claude to do it for me? It needed a bit of help here and there but it did pretty well.

Now my codebase has \~5000 less lines of code but the same functionality. These are the kind of tasks I like using AI for the most.

There are also other aspects, not strictly related to code, where I use AI. For example, all of the images in this component were generated by ChatGPT.

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FRiver-guvWDXsXm603mnNcSVE5utdb9htSIu.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FBeach2-7Ju0JFtz96qCZiKcJ2pjqMr6t75ePt.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FCity-NiSzH3I7jvF6uWSDgH6Ir6wXsC1JOD.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FSnow-uIsjugzCxFMaqYgFY9csfkRKX8DqfD.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FDesert2-SnMCtUDmKAIxfkFnfoLRh7s7lJFsPs.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FFuji2-oUusgSaGjrc3pkdasZp2w3y2bnhiSV.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FVolcano-uhgiBEcTnisoehyei3DRWuk4yAztgv.webp&w=2048&q=75)

![](https://jakub.kr/_next/image?url=https%3A%2F%2Foiszjiwtfc65cwa2.public.blob.vercel-storage.com%2Fimage-spotlight%2FRain-YEa7DVUUkQ6wAwctRA4Rz99XhcUzG4.webp&w=2048&q=75)

## Conclusion

AI has changed the way I work on a day-to-day basis. On one hand it's fantastic, you can get things done faster than ever and you can experiment more than ever.

On the other hand, there is also more slop than ever. Handing-off everything you do to an agent and _outsourcing your thinking can hurt you in the long run_. That's why I always try to use AI to only accelerate bits and pieces that are tedious or time-consuming but never to replace my thinking.

With such a vast amount of knowledge at our fingertips and the barrier to entry being lower than ever, quality and craftsmanship will be more important than ever. Design and user-experience _will become the [product](https://x.com/lukeshiels/status/2009313575349977295)_.

Some of the things mentioned in the article are obvious to some, but hopefully helpful to others. I'm not an AI expert by any means, I just wanted to share my workflow.

If you have any tips or tricks, feel free to [shoot me an email](mailto:jakub@kbo.sk). I'd also recommend watching [this video](https://x.com/leerob/status/2011810357942084085) by [Lee Robinson](https://x.com/leerob) about a lot of the concepts outlined in this article.

Thanks to [Hana](https://www.linkedin.com/in/hanaholcikova/) and [Luke](https://x.com/lukeshiels) for proofreading and giving feedback on this article.

## More

In case you have any questions reach me at [jakub@kbo.sk](mailto:jakub@kbo.sk), see more of my work on [Twitter](https://x.com/jakubkrehel) or subscribe to my newsletter.

hidden text to trigger resize events if fonts change

---

# [五条0122] 继响应式设计后，智能体将颠覆产品体验？
发布日期：2026/01/15

In his seminal 2010 article on [responsive design](https://alistapart.com/article/responsive-web-design/), Ethan Marcotte described a new way of thinking about web user interfaces that moved beyond a fixed frame. Reacting to his clients’ requests for an “iPhone version” of their websites, he suggested instead a single experience that could _respond_ to the user’s device. Page elements could use a different set of design properties for large screens and small, transforming their form to match the capabilities of the hardware. Crucially, on simpler devices, elements could be hidden entirely if deemed non-essential.

As this technique became popular among designers and product teams, they quickly realized that it was more challenging to start with the “full” version and pare it down. Instead, by starting with the _least_ capable device, you could boil the product offering down to its essence and present those primitives in the limited screen real estate available. This soon led to an acknowledgment of the irony of responsive design: That the least capable devices often had the most effective user experience, as they were most closely aligned with user needs.

We came to realize that responsive design wasn’t just about layouts, it was about forcing organizations to confront what actually mattered. But organizations, it turns out, cannot resist the temptation to use available UI space for promotions or expressions of their org chart, all of which almost always get in the way of what users are trying to do. That this still remains true all these years later is astounding; the difference in the UX of apps versus websites for banks, airlines, and ecommerce is stark evidence.

## Towards a More Radical Responsiveness

I believe we’re at the beginning of a new era of further crystallization, and it feels both more disruptive and more enabling than the shift to responsive design 15 years ago. Earlier last year, foundation model providers released coding agents that developers could install and use directly from their command line terminals. These agents, including Claude Code and OpenAI Codex, introduced two seemingly small advances that would turn out to be incredibly significant.

First, they could iterate. Unlike earlier chatbot interfaces, these agents would continue to work and refine their output, looping over a plan until its goals had been achieved. Second, they were given access to tools. They could now call standard Unix utilities like curl for loading web pages or grep for searching file contents. A few months later, the underlying models were updated specifically to take advantage of this kind of tool use and to stay more focused on plan-following. With those pieces in place, developers on the frontier of AI-assisted engineering began sharing new workflows that felt genuinely different and everything sped up again.

Recently, many non-developers, myself included, have found that using Claude Code with files locally can be an incredibly effective way to get work done. My social feed is filled with people sharing their use cases: setting the agent to work on an Obsidian vault, managing email and calendars, finally getting value from smart home devices.

## Primitives All the Way Down

The key to all of this is the tools: small, simple command-line apps with clear, direct documentation. Despite the promise of MCP servers, Claude Code with access to [iCalBuddy](https://hasseg.org/icalBuddy/) is a much faster and more effective way to ask for help organizing next week’s schedule. Codex is an expert with `gh`, the CLI app for GitHub, and can quickly summarize and organize your next sprint’s outstanding issues. Both can quickly navigate through 1Password with the `op` command to avoid spewing access tokens everywhere. These apps so clearly expose the primitives of the systems behind them. An agent looping iteratively while stringing dozens of these composable tools together starts to feel like a very new way of working.

This is the next step in responsive design and the blueprints are lurking all around us. The coding agents now let you add skills — simple descriptions of how to accomplish a task written in natural language. You might explain to your agent how to get data from an internal API to help you create your quarterly update, for example. But if you really want a great look at what the future may hold, try this: on an iPhone or Mac, open the Shortcuts app and start searching through the available commands exposed by each app you have installed. There, you’ll find a remarkably clear visualization of all the atomic components of an app’s capabilities. All the nouns and verbs.

![](https://veen.com/jeff/images/shortcuts.png)

Three screenshots show shortcut actions for iOS apps Kayak, Contacts, and Wells Fargo

None of _that_ works with agents quite yet, but I think this is the clearest glimpse into what apps become in a world of advanced agentic workflows. Here in three screens are the actions we can take when communicating with friends, planning our vacations, and managing our money. This is what it looks like when apps are honest about what they can do. And a step further: all the APIs for all the enterprise SAAS and personal productivity apps, lurking behind developer documentation, waiting to be unlocked. What is a “seat” and what is a “user” when we send our agents to ask questions and make requests? This isn’t just app design, but what _businesses_ become as we collectively lean harder into agents.

## Clarity as Competitive Advantage

This may seem counterintuitive, but my career in user experience design makes me even more excited about a potential future like this. If agentic workflows strip back applications to their bare essence, then it makes sense that they’d present what they find in UI components crafted on the fly. Perfect customization, whether you’re accessing the capability on a large display or through your AirPods, in English or Cantonese, wrapped in shadcn or Chakra UI — your choice!

An agentic future elevates design into pure strategy, which is what the best designers have wanted all along. Crafting a great user experience is impossible if the way in which the business expresses its capabilities is muddied, vague or deceptive. So the best have always pushed for more authority, more access to the conversations where the real decisions are being made in their organizations. To gain that access while maintaining advocacy for humans using the products has always been my goal as a designer.

This takes a pretty big leap of faith for those practicing design. I see the same happening for engineers waking up to the fact that they’ll soon stop typing code, and not long after that, stop reading it. Many of us are imagining futures in which aspects of the labor we love are quickly slipping away. But I’ll argue it’s what we’ve been after all along: Responsive designs that mold to our customers and patients and citizens in ways that ask businesses and institutions to express to us exactly what they have to offer.

If an agent used your product tomorrow, what truths would it uncover about your organization?

hidden text to trigger resize events if fonts change

---

# [五条0119] 谷歌Gemini设计揭秘：如何让AI更可信？
发布日期：2026/01/19

How do you build trust in a tool that’s always evolving? Our AI assistant, Gemini, is constantly learning and adapting, which means traditional design methodologies (mostly linear, predictable, and rooted in control) have to be set aside.

Our designers took on the challenge of charting a new digital landscape. How do you bridge conceptual gaps around AI, especially around how Gemini simplifies information and expands ideas? How do you invite users to explore, play, and search, so they can build confidence using an ever-shifting tool? These questions were guided by a core set of goals: to make Gemini feel intuitive, immersive, approachable, aspirational — and, above all, trustworthy.

While Gemini’s [sparkle icon](https://design.google/library/ai-sparkle-icon-research-pozos-schmidt) may be the most ubiquitous visual element within the Gemini brand, there are many other elements of the product’s illustration system to be explored. In this deep dive, the designers behind the system pull back the curtain to reveal gradients, responsive containers, and intentional motion that seamlessly work together to create a sense of magic, clarity, and familiarity within the Gemini experience.

Navigating uncharted design territory isn’t new. Consider designer Susan Kare, who pioneered the original Macintosh interface. Using simple visual metaphors, she made abstract digital processes tangible and intuitive for new users: a trash can, a paintbrush, a smiling computer face. Her icons weren’t just pixels; they were bridges between human understanding and machine logic. Gemini faces a similar challenge around accessibility, visibility, and alleviating potential concerns. What is Gemini’s equivalent of Kare’s smiling computer face?

The design team landed on gradients, which gently guide users into the new collaborative world with Gemini. It’s an amorphous, adaptable approach, unlike the static nature of past digital assistants (many of us remember Microsoft’s Clippy), yet it inspires a similar sense of discoverability. Gradients might be much more about energy than “objectness,” like Kare’s illustrations (a trash can is a thing, a gradient is a vibe), but they infuse a spirit and directionality into Gemini.

A solid blue pill shape centered on a vertical line representing a pressed power button state. A soft, diffused blue gradient glowing from a power button icon, suggesting an active, energized state. A solid green curved shape nestled in the bottom-left corner of a frame, indicating a swipe gesture. A vibrant, multi-colored blurred shape pulsing from a corner, representing an active swipe-to-activate gesture.

Simple sequences were explored to guide users toward activating Gemini. These included pressing and holding the power button or swiping from a corner.

Gradients are central to Gemini’s visual storytelling, acting as context builders that guide users through the product experience. Designed to convey a transfer of energy and directional momentum, they feature sharp, almost opaque leading edges that diffuse at the tail, acting as clear visual pointers to direct user attention toward what’s most important.

To make the system feel alive, our designers wanted to visualize Gemini’s process of active thinking and synthesis, which helps personify the AI assistant rather than rendering it impenetrable.

Gemini’s visual language needed to support its own identity while integrating with Google’s existing aesthetic. The design team looked to the company’s familiar logo for inspiration, focusing on the fundamental shape of the circle. This choice was deliberate, as circles tend to convey simplicity, harmony, and comfort. Even Gemini’s own logo is thoughtfully constructed from the negative space of four adjoining circles.

We explored several foundational shapes to represent Gemini’s “thinking state,” drawing heavily from Google’s design heritage. We experimented with the iconic four-color dots — a symbol of the brand’s rounded, optimistic language — and various Material shapes historically tied to voice and Android’s system UI. By leveraging these established elements, we were able to ground the new framework in a familiar history while ultimately evolving them into a more dynamic, fluid expression of AI intelligence.

Many of Gemini’s interface elements, such as buttons and containers, echo the circle through their rounded corners. These subtle visual cues bring continuity with other Google applications. But perhaps the most impactful use of the circle is how it contains and sculpts Gemini’s signature color gradients. Here, shape acts as a dynamic vessel, concentrating energy into a sharp point before allowing it to blossom outwards—a visual metaphor for Gemini’s ability to process and expand ideas.

People expect consistency from digital systems; they want to learn once and apply that knowledge repeatedly. The challenge for the Gemini illustration team has been to cut through that dynamism, creating a new, yet familiar, visual language.

The illustrations are meant to have a “warm, spatial, rounded quality,” says Anna Sera Garcia, design lead. “We’re always considering how to depict our UI in a way that feels optimistic, delightful, playful, yet also sophisticated. There’s something ethereal — that kind of in-between fuzzy space that reflects our nonlinear process for ideation.” The aim was to make sure the illustrations felt intuitive and frictionless, which is crucial for a product like Gemini that’s so new, revolutionary, and always changing.

Movement in Gemini is not merely decorative; it’s an essential guiding element. Each animation has a defined start and end point, creating a sense of directional flow that mirrors user actions. This sense of responsiveness helps users intuitively understand that the system is working with them. Inner activity within the motion conveys thinking, analysis, and intelligence, making Gemini’s processing feel more transparent. Motion allows users to see information coming together, visualizing Gemini’s conversations and listening abilities.

“Whether it’s introducing haptics or subtle shifts in how we communicate, the aim is to make the interaction feel easy and intuitive for users,” says Garcia. “It’s interesting to look back at how technology has shaped the ways we converse and learn. We’re trying to unlock those foundational principles that feel intuitive for this new set of interactions.” These principles work as context builders where one thing leads to the next, helping users feel guided rather than lost.

## Embracing softness in the face of change

Gemini’s solutions feel cool, calm, and considered, by concealing the struggles and challenges of their creation. Garcia notes, “There’s a sense of play and camaraderie that happens when brainstorming, knowing deliverables will change.” Our onboarding illustrations are constantly being refined, building on the elements that already exist.

This process highlights the importance of a foundational experience that forgives mistakes, anticipates confusion, and invites exploration. When a system is hard to approach, the design must be soft. This softness — conveyed through guided, pulsing gradient shapes, clear language, and transparent signaling — allows users to engage with the new system feeling secure and supported. The gradient can be many things through its animations: aspirational and uplifting, directional and instructional. But they remain soft and direct, and always looking forward; they’re deeply connected to the Google brand with room to grow, like the personified gradient, rippling and responding to voice.

## The designer as cartographer

Designing illustrations for Gemini is like charting a continuously evolving map. Designers aren’t creating a fixed tool; they’re rigorously composing a relational experience that adapts as much as it informs. How do you build trust with a tool that won’t look the same tomorrow? The user doesn’t need the system to be perfect. They need it to be thoughtfully imperfect — and this, now, is the designer’s job, as we move into future evolutions of Gemini.

_Special thanks to Buck, our creative partner and agency, Ivan Witteborg, Namroac Doan and Andy Stewart for their contributions to this work._

hidden text to trigger resize events if fonts change

---

# [五条0120] 阶跃AI Agent发布，你的桌面超能助理
发布日期：2026/01/19

Agent 工具领域越来越热闹了！阶跃也早就开始了这方面的探索，我们希望打造“会做事、总在场、有记忆、能进化”的终端Agent，帮大家更好的工作和生活。

去年 9 月，我们发布了[阶跃AI桌面伙伴 Mac 版](https://mp.weixin.qq.com/s?%5F%5Fbiz=MzkyNTYxNzg5Mg==&mid=2247486553&idx=1&sn=01a5f27b3960c1084c2d904e04dfcdb2&scene=21#wechat%5Fredirect)，你可以叫它“小跃”，它时刻在线、可以同时执行多个任务。小跃连接了本地操作系统，支持查看和管理本地文件、访问互联网、执行复杂任务，支持通过“妙计”复用操作步骤，也支持设置“定时任务”到点自动执行。

小跃上线后，我们收到了大量“催更” 需求：

“能不能直接帮我操作这个软件？”

“我上午明明看过一个网页，现在怎么都找不到了。”

“我现在正在写的这个文档，AI 能不能直接‘看见’？”

当然，我们收到最多的反馈是：“Windows 版什么时候出？！”

今天，我们带着这份“共创成绩单”回来了：

**阶跃 AI 桌面伙伴 Windows 版正式上线！还有重要升级！**

![](https://mmbiz.qpic.cn/mmbiz_png/0LDJQiaWfxznFo1licu7O8jLn5eHbDrDMhHvu3rhnt4saZE8LjW0huE0ISib4abvy6nfficvqkQFQgfprvy1UxjVxw/640?wx_fmt=png&from=appmsg&tp=webp&wxfrom=5&wx_lazy=1)

访问 www.stepfun.com/download 即可下载

#### 1\. 支持调用第三方工具（MCP）

支持通过 MCP 方式，调用包括 Excel、QQ邮箱、飞书、钉钉、Notion、高德等 16 款工具，且用户可根据需求自行添加更多 MCP 工具。一句指令，跨应用操作，让小跃成为“高效实干派”。

案例展示：自动发邮件

_HR用小跃自动群发个性化邮件。只需添加当月全员工资单和文案模板，小跃会自动把每位员工的工资条群发到对应员工邮箱，为每位员工适配个人模板。_

，时长00:40

案例展示：赛事数据分析

_用小跃获取赛事数据并分析，收集包括周末曼市德比在内的曼联最近4场英超比赛数据，进行复盘并做成网页数据看板，直观呈现。_

，时长00:24

#### **2\. 全局记忆，你的第二大脑**

想知道自己这一整天在电脑上都干了什么？或者想找回“上午看过的那个网页”？

现在，你不用再刻意记录。小跃的「全局记忆」功能可以帮你自动整理电脑活动轨迹，并生成每日复盘报告，形成一个可回溯的个人工作记忆层。而且，相关信息均储存在用户本地设备上，保护数据安全。

![](https://mmbiz.qpic.cn/mmbiz_png/0LDJQiaWfxznFo1licu7O8jLn5eHbDrDMhR61mET3RicC53DMEyGTNFvLUNB1TDHicjXWFS7zYia0huuawHN15uJB4w/640?wx_fmt=png&from=appmsg#imgIndex=1)

注：目前仅 Mac 版本支持

3\. 同步应用，“看见”你的当前窗口

过去你可能需要这样用 AI： 复制 → 粘贴 → 描述背景 → 再下指令

现在，只需点击悬浮球，小跃能自动识别你当前正在使用的文件、网页或应用，并将其作为“上下文”填入输入框。告别复制粘贴，实现“所见即所问”！

，时长00:09

注：目前仅 Mac 版本支持

特别活动：寻找全网最会“用AI”的你

小跃的进化，一半来自代码，一半来自你的脑洞。为了感谢这段时间陪伴我们捉虫、提建议的朋友，我们正式发起：【阶跃 AI 帮我搞定了这件事】主题活动，欢迎大家参与！

从 Mac 到 Windows，从单一对话到全局记忆，小跃的每一步进化都离不开大家的催更和反馈。

我们依然在路上，如果你在体验中遇到任何问题，或者对新功能有更多脑洞，欢迎进入社群直接“开麦”。

小跃，由我们发布，由你定义。

![](https://mmbiz.qpic.cn/mmbiz_png/0LDJQiaWfxznFo1licu7O8jLn5eHbDrDMhe1HthzA0O830ibOGLibePmsS7VhpJIPGdSObGYYdFwwQu3x6nias2TYiaA/640?wx_fmt=png&from=appmsg#imgIndex=2)

延展阅读

[](https://mp.weixin.qq.com/s?%5F%5Fbiz=MzkyNTYxNzg5Mg==&mid=2247486748&idx=1&sn=6c09f0537deccbb562f2db624f7141bd&scene=21#wechat%5Fredirect)

[![](https://mmbiz.qpic.cn/mmbiz_png/0LDJQiaWfxznPPYwVX0aW0FGBbp2fn8hGtCIwD6iaaCiauXemJFNtj9YQiaU90JpnAMP5ib73H0oicu7c7PHYr1skLicg/640?wx_fmt=png&from=appmsg#imgIndex=3)](https://mp.weixin.qq.com/s?%5F%5Fbiz=MzkyNTYxNzg5Mg==&mid=2247486748&idx=1&sn=6c09f0537deccbb562f2db624f7141bd&scene=21#wechat%5Fredirect)

[](https://mp.weixin.qq.com/s?%5F%5Fbiz=MzkyNTYxNzg5Mg==&mid=2247486695&idx=1&sn=23a3f844dbf190c8834199a15a8867eb&scene=21#wechat%5Fredirect)

[![](https://mmbiz.qpic.cn/mmbiz_jpg/0LDJQiaWfxznPPYwVX0aW0FGBbp2fn8hGq73pGmWFClFia3ZZx2sqavu9yj4jW79s5ou3hXF5oicpstrdofOUH0Vw/640?wx_fmt=jpeg#imgIndex=4)](https://mp.weixin.qq.com/s?%5F%5Fbiz=MzkyNTYxNzg5Mg==&mid=2247486695&idx=1&sn=23a3f844dbf190c8834199a15a8867eb&scene=21#wechat%5Fredirect)

hidden text to trigger resize events if fonts change