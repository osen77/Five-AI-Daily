# [五条0109] 模型设计师：不止于UI的设计新物种
发布日期：2026/01/06

with Barron Webster

Hey there, we're [Ryan](https://x.com/Flomerboy), [Federico](https://x.com/federicovillaw), and [Robin](https://x.com/robinconline), a trio designers who have noticed an undercurrent of change in the design world. AI is taking over software, and as it does, the demands on its designers are changing. AI is squishy and has a mind of its own, so designing AI-first products feels less like designing for print or HTML, and more like working with some kind of alien intelligence that crash-landed on Earth. The skills needed by designers are rapidly changing, and every day a new model or tool drops that turns everything on its head.

So we started AI Design Field Guide to serve as both a resource for our peers in the field, and also a time capsule of this unique time where we're all figuring out how this new kind of work is done in real time. Each article is an interview with a designer working in the field who has a unique perspective on what's happening.

Our first interview is with Barron Webster. We knew we had to talk to him first because he's been elbow-deep on AI products for over 8 years (AI years are kind of like dog years). If there's anyone that could see through the hype cycles and into the future, it would be him. Early in his career, he designed [Teachable Machine](https://teachablemachine.withgoogle.com/) at [Google Creative Lab](https://www.creativelab5.com/), the first consumer tool for training AI models. It launched in 2017\. After Google, he joined [Replit](https://replit.com/) to work on their AI features, which were the force that drove them to grow from a startup into a unicorn.

He recently joined [Figma](https://www.figma.com/) as one of the world's first model designers – a new hybrid role for people who want to get their hands exceptionally dirty with LLMs. The emergence of this new role seems like proof of this change we're noticing. So, we thought that unpacking it with Barron was the perfect way to kickstart our conversation about how design is changing in the age of AI.

I sit with the AI research team at Figma, and they hired me for two main reasons. For one, they're reaching a point where they're getting all of the juice that they can squeeze out of the foundation models, and it's not good enough. A lot of Figma's data is in a proprietary format that may never see the light of day, so foundation models aren't particularly good at working with it. Part of my job is bridging that gap.

The other big part is building new tools and AI-first thinking to the design org. You know, Figma's a big company – lots of designers working on parts of the product who haven't designed AI experiences before. Right now, there isn't much tooling, inside or outside, that makes designing those experiences easy, fun, or even possible. AI feature design looks different from traditional product design.

There are steps that designers can take a lot earlier in the process to prototype the core of the AI feature, before getting into the UI. If you're designing UI for something that you haven't played with, the risk is that you're designing UI for a perfect case that isn't representative of how it will work. So one of the things that I'm excited about is building the tools that let designers prototype and play with that part of the process early on without having to become an engineer.

It's like the AI is [Cthulhu](https://swetlana-ai.medium.com/the-shoggoth-meme-hp-lovecraft-meets-ai-60fc44692e98), and the UI is this mech suit. And the goal is to get the designer to understand Cthulhu's anatomy before designing the mech suit so that the mech suit doesn't just explode when Cthulhu puts it on.

The things that I'm most excited about are tools that allow designers to manipulate and run eval cases in a fast feedback loop and in the native format to the material that they're working in. Imagine you're working in a figma file and you try an AI feature and it doesn't work, you should be able to immediately add that as a test case to your evals. What if I adjust the system prompt for the tool that didn't work? What if I try a different model on this one?

Part of the reason that it's so hard right now is that the feedback loop is so slow. Fundamentally, all good design tools remove or shrink the feedback loop. There's a lot of room to improve. I don't know if you are familiar with building your own eval sets, but a lot of it feels like you're doing a lot of manual labor to just shuffle data around.

The other part of the job that I forgot to mention, is thinking about how to make the AI features more differentiated at Figma. It's a design platform. So you'd expect that the outputs will be better designed than, like, [Claude Code](https://github.com/anthropics/claude-code) or [Cursor](https://cursor.com/). How do you ensure that the outputs are of the highest quality? I think a lot of that will look like targeted eval strategies and finding proxies for what we consider good design, which is a really fun and interesting kind of art school, philosophical question.

Yeah, if "design is how it works", increasingly the "how it works" part is governed by the weights of the models that are driving these experiences. So design for these sorts of experiences should involve getting your hands dirty in the model weights.

They were looking for someone to think about this space. I started talking to them this summer, and there were a bunch of problems that I knew that they had, but I don't think it was clear to anyone, including me, which of them was the top priority. I think I'm, to some degree, the canary in the coal mine, which has been fun and chaotic, because I've been jumping from context to context and team to team. It's a huge company with eight different products now, and so there's a lot of stuff happening.

So you've clearly been deep in this space for a while. You're probably one of the designers who's been working with AI the longest – could you share your first introduction to what we now call AI?

There are two points in time that stand out. One was at [RISD](https://www.risd.edu/) around 2014 or 2015, in a class called Computer Utopias by Chris Novello. This was pre-LLM, when machine learning research was more about classifiers. We surveyed modern digital products and technology, and the most exciting things were image classification models—the ability to feed data into a model and get pretty good image segmentation and classification. This was driving features like [Snapchat](https://www.snapchat.com/) face filters and Google's image search.

Alongside image models, content moderation and recommendation systems were big. This was the heyday of Facebook, Twitter, and Cambridge Analytica. The idea that you could design a system to show users curated content based on their consumption patterns was a huge shift. From 2013 to 2017, platforms like YouTube, Facebook, and Twitter inventing the algorithmic feed created a new material to design around, moving away from just subscribing to topics or following people. It was a contentious topic at the time, but that was my first theoretical exposure as a student.

The second big moment was from 2016 to 2018 at Google's Creative Lab, working on [Google Lens](https://lens.google/), [Google Assistant](https://assistant.google.com/), and [Teachable Machine](https://teachablemachine.withgoogle.com/). Nearly all our projects applied some form of model innovation. For Lens, it was improved image segmentation and classification. For Assistant, pre-LLM, the innovation was mainly in voice-to-text and text-to-action processing. Teachable Machine was pure image classification. This era was interesting because it wasn't about text generation; it was about using models to sort or annotate stuff that already existed.

You know a lot of people say, I want the robot to do my laundry and dishes, and it's really easy to forget that for the longest time AI was just for processing your content and only recently did it start to write poetry.

Haha, yeah. I remember we even did a promotion about a cucumber farmer in Japan using [TensorFlow](https://www.tensorflow.org/) to sort cucumbers—it's funny because it's just a very practical use case where a simple classifier likely still outperforms an LLM today.

Yeah I guess "multi-modal models." But I also think we're starting to move into a world where users mostly interact with "model systems", with agentic systems or things like Deep Research.

Were there any projects you've worked on over the years that really surprised you or changed how you think about designing for AI? What did you learn?

I spent over three years at [Replit](https://replit.com/), a collaborative web-based programming environment. I was hired partly to evaluate where we could use AI, as they had no AI features at the time. During my three years there, the models kept getting progressively better. So we were constantly looking for ways to add AI functionality that leveraged the models' new capabilities in a way that was both useful to our audience and reliable.

It started with basic, manually triggered features like selecting code for an AI explanation or generating code in an existing file. With each new feature, user expectations rose. We were on this cycle of trying to meet their desires, where each release showed them a future they thought should be possible, but the models weren't quite there yet. For instance, when we allowed generating code snippets, users asked for entire files or projects. Once we could do that, they wanted specific edits. Then, they wanted to start from scratch.

We knew what kinds of features we wanted to support and would try them with existing models. If it didn't work, we'd pause. When new foundation models came out, we'd try again.

Programming environments have specific product constraints. Even if a model is great at writing code, you have to figure out how to get it to edit code in the right place. I remember until around [Sonnet 3.5](https://www.anthropic.com/claude/sonnet) came out, models weren't good with line numbers. We had to devise hacky ways to ensure edits were correct, didn't duplicate content, or properly replaced functions. These weren't AI innovation problems per se, but product scaffolding issues to handle model limitations. In hindsight, much of that work was only relevant for six months to a year until a new model obviated it.

That resonates with me. There's this question of "Should we build this feature that's technically possible today but would be obviated by a model release in a few months, or should we build some other feature which just gets better as the models do. It seems like a part of AI design is this dance of testing out new models, prototyping with them, seeing what's possible, what's not, thinking about which features will stand the test of time. Was there anything you did that stood out to you as particularly impactful? It's not typical design work you do in Figma.

A big part of what design and research are doing now is ensuring the team stays focused on the right thing. It's easy to start building a feature, encounter technical hurdles, solve them, find more, and end up with something technically functional but overly complicated for the user or drifted from the original goal.

A concrete example is when we were working on the Replit agent, which automatically created files and wrote code. A huge technical problem was getting the agent to test the applications it built. For instance, verifying if a login page worked. The engineering side saw this as a cool technical problem: spin up a sandbox, build screenshot functionality, feed screenshots to a multimodal model to decide where to click and type—essentially pseudo-computer use by the model. It's satisfying to go down that rabbit hole.

However, myself and another engineer proposed: what if we just showed the user the website and asked them to test it? We offloaded validation and testing to the user, skipping that entire complex technical problem. Having someone in the room focused on the user problem, not just the technical one, can help you skip or simplify many things, even if the solution is less technically exciting.

Yeah, that's a good example of stepping back and asking 'do we actually need to solve this?' I imagine it's hard to know where to draw that line of which tasks to automate, eval, and test, and which ones to leave up to the user to figure out. If you'll allow another analogy - creating AI products is kind of like making little robot assistants that are going out into the world and we don't know what people will ask them to do. Should we make sure they can handle someone who changes their mind halfway through? Or someone who asks three questions at once? Should we make sure they know the heimlich maneuver? How did you navigate those decisions of what needed to be tested and what didn't?

At Replit, while there were subsets of users, we generally knew what our platform was capable of and what users wanted. So, we focused on features that helped them achieve their goals faster with AI, compared to typing all the code themselves.

Building an AI product from scratch today is harder. Traditional startup wisdom is to start niche, find 100 users who love your product, then expand. General chatbots like [ChatGPT](https://chat.openai.com/) have seen success with the opposite approach, build a general interface that can be used for anything and… see what happens. That might be a one-time fluke due to the new technology. If I were building an AI product now, I probably wouldn't start by targeting a general use case unless I had a significant technical or distribution edge, like [Meta](https://ai.meta.com/) has with [Llama](https://llama.meta.com/).

This evolved... For our first AI feature, code explanation, it was just me and one engineer, pre-[LangChain](https://www.langchain.com/) and widespread evals. We tested it like other platform features: launch internally, have staff use it (most Replit staff are programmers, so they were good evaluators), make adjustments, then launch to a small volunteer group of users (maybe 1-5% of the user base).

We'd interview these users, pull analytics to see who used it most and retained, and talk to those who tried it once and stopped. This is a traditional agile startup approach: incremental rollouts, user conversations, willingness to roll back. For code explanations, judging "good enough" was subjective. Staff would review explanations for correctness.

Once AI started authoring code, tracking success became easier. We looked at signals: Did the user accept the AI-generated code (shown temporarily before insertion)? Did the inserted code cause lint errors? Did they revert or delete the code soon after? The programming space is fortunate because it's easier to validate if something works; you can run the program or it doesn't. It's trickier with, say, an email authoring tool where success is less binary.

Beyond the quantitative metrics that assessed the performance of those features - did you have other ways of detecting product-market fit? How did you know if the features were solving problems for users or not?

Yeah, pre-AI product strategy was quite different. You'd have plans, an existing user base, and you'd strategize abstractly about expanding markets or categories. With AI's rapid changes, our strategies at Replit were much more reactive.

For example, when I started, we focused on education due to strong product-market fit, especially post-COVID for remote teaching. But as AI features improved, we faced a dilemma: indie developers and hackers loved the AI, while teachers often disliked it because students could bypass learning fundamentals.

This happened a couple times, we found product-market fit more organically than strategically. When we released the Replit agent, our last major AI product, we didn't really know who it was for. It felt more reactive than some of our top-down projects like enterprise plans and teams. The more successful projects were ones where we just shipped features and saw what happened.

Later on, we discovered its users—often ops people at tech companies needing to ingest sales data or build dashboards, similar to [Zapier](https://zapier.com/) or [Retool](https://retool.com/) users—by releasing the tool and then talking to those who adopted it.

Probably now, they have a more strategic approach, now that Replit Agent has found a fair amount of product-market fit.

It does feel like it goes against the conventional wisdom. "Build it and they will come" is usually seen as something that doesn't happen, but it seems to keep on happening! It's a humility check for designers, isn't it? Engineering is kind of eating into lower-level design work. Design is moving up the chain. Design used to be about - how does the feature work? But more and more, the models are in charge of how the product works, and designers are more like - where do we focus?

Yeah or like, how do we expose that. In the last era of technology, a lot of the tech was fundamentally pretty simple CRUD, and design played a role in figuring out how to scaffold information to solve a specific problem for the user. But today, it feels like a lot of the questions are just like - is this going to work at all?

That makes me wonder - is that just a reflection of how new we are as a discipline? Like if we're basically saying "we don't know what's going to work until the end" maybe that points to a lack of tools or methodologies.

Well, speaking of "is this going to work at all" - what about evals? Did those play a key role in your decision making?

For the first two years at Replit, we didn't do many evals. The practice wasn't really widespread. With the agent, we leaned into them more, but primarily as indicators for product development rather than for validating our own products. For instance, when a new model comes out (e.g. [Llama 3](https://llama.meta.com/)), we'd look at its performance on programming evals to decide if we should test it in our app.

More recently, at [Sandbar](https://sandbar.inc/), I spent a lot of time writing evals, particularly for model personality. There are broad industry benchmark evals for basic stuff like, not saying anything offensive, but building specific evals for things your product uniquely cares about is part of this new design work. The workflow was heavily:

If we didn't have evals, we'd have to do a lot more manual labor to verify the AI is working well. Evals are a faster way to test if you're meeting desired characteristics. For example at [Sandbar](https://sandbar.inc/), we cared that if the model didn't know an answer, it should ask a single, specific clarifying question rather than hallucinate. We had an eval for that. We had others: don't ask more than one question at a time, keep answers concise (with exceptions).

The tricky thing about evals too is, at least in my experience, a lot of the time, the cause of poor eval results isn't necessarily poor performance, but a badly written eval. Like if you had an eval saying the model should be very concise, and the user says "oh my mom died" and the AI says "That sucks" maybe that would result in a great eval score "Very concise! 10 out of 10!", but maybe that's not actually what you want as a user in that situation. Maybe it should say "Oh I'm sorry for your loss" even though that's not as concise.

So there's this question of, how much of your time do you spend writing good evals. Evals are kind of like the vital signs of seeing a doctor. Like does this person need to go to the ER?

Yeah I mean in our case, we would have an eval covering empathy too, so you want to have a set of evals covering all the things you care about, not just some of them. For us, evals were primarily for avoiding regressions. We had characteristics we wanted to meet, and if a model or prompt change caused a regression, evals signaled where to massage the system to get back to baseline. I think of it like test coverage in programming.

It's interesting, in traditional programming there's this idea of test-driven development where you write the tests first, and then write the code that needs to pass the tests. I haven't seen the equivalent of that very much in AI engineering, where you write the evals first. I think I've seen a couple papers but not in production.

There might be a future job like, eval designer, which is like a design systems role that designs dashboards for the rest of the team to understand how the AI is performing.

It's a bit like a service design orientation. Imagine you were designing a hotel experience, and you have to decide, how are we going to assess our staff? Is it primarily about how friendly they are, or how proactive?

Totally. Barron, can you speak to times where you've tried things and maybe they don't hit the acceptance marks that you guys are looking for. How did you decide when a feature wasn't good enough to look to ship, versus, let's just go with it and we can tune it as we go.

One of the main things was sycophancy. This was probably one of the hardest things to write evals for - the idea that the model should push back on you in cases where you know you could use it. Some of the time, it's at that point it becomes more of a product and design decision to orient the team on what an acceptable failure rate is, and that becomes part of the design philosophy of your product.

Also, it's easy to feel like, Oh, we did all this testing, it didn't work, and then the new model came out and it was fixed. Was that time wasted? At the same time I'd say - don't sell yourself short on the work that was done to understand the feature, what makes it good or bad, such that when the new model did drop and you tested it, you had conviction that hey, this is good enough.

Yeah. I think at Replit and Figma so far, the idea of sycophancy doesn't really make its way into the product very often. At [Sandbar](https://sandbar.inc/), it's a very open-ended conversational interface as the backbone of the product, and in that sense, a philosophical experience we wanted to avoid was continually agreeing with the user and feeding their ego.

At Figma, one idea we've been thinking about is "design critique as a service"—you ask an AI to critique your design—and that raises interesting questions about the personality of that system. Is it something you opt into, like choosing a "[Dieter Rams](https://en.wikipedia.org/wiki/Dieter%5FRams)" attitude, or do we have a default? And do we focus on accessibility or contrast issues—more objective feedback—or aim for something broader? I'm not sure how much that will make its way into the actual product experience.

How would you like the Evals field to evolve overall? Like, what would be really helpful for you as someone practicing in a field that's so new? Is it like an open tool where people can post eval sets or parameters that they're setting? And do you think the industry needs that? Right now it seems like a lot of stuff is proprietary.

The kinds of tools I find myself wishing for, and hopefully can work on at Figma, are ones that reduce the iteration speed for creating evals. It's so painful right now, and I feel like everyone that's working on evals, like, has to basically do this work. How do I map it? What's the format? What pipeline is it running in? And, like, hooking the output of the pipeline up to an interface where you can see everything in one place. There are tools out there that are, like, pretty good at this for text, but not so much for other formats. There are some interesting platforms out there that are like pseudo evals, like [Design Arena](https://www.designarena.ai/), or, like, I forget what the other one is…

Yeah. Those are cool. Maybe they're other interesting formats like that, which designers can get involved with. I would love to be able to do something similar but directly in figma files, including commenting on issues and stuff. I want to be able to quickly create sets of tests that I want to run, kick them all off, get like 100 responses, and do it again in like 30 seconds. We have all of those pieces kind of working, but it takes way too long to do all of them.

You know how, like, an architect can look at a building and guess what software it was designed in? I'm just realizing that Barron is essentially creating the fingerprint of like, the next era of web design. Whichever evals you pick, people will be able to guess that it's a figma-AI-created website…

I think that's an interesting tension for all of AI design - how much do we want to assert our own point of view as the tool-creator versus adaptive to what the user wants. If the user is really going for a web 1.0 aesthetic with green text on a red background, should the system lean into that and match it, or should it adhere to our idea of what good design is?

Shifting gears a bit, besides the work of building products with AI in them, there's also the process of designing the model itself. In a sense, LLMs are a new kind of "computer", with much of the "programs" being in the weights of the model itself. When it comes to actual model creation, how do you think a designer can create the most value?

I've experienced two main ways – fine tuning vs. training from scratch. If you're training a model from scratch, a designer's biggest impact is pointing the organization to where user needs are greatest and pain points are most acute. Then, work with engineering to find the right approach. At Replit, we trained a custom model on Python for common, simple code errors because we saw significant user frustration there. The design and research input was highlighting these user problems. We were less involved in the actual training, more in defining the problem and then figuring out how to apply the trained model in the product.

The other approach is fine-tuning an existing model. If you have an existing model, product, and evals, and you want to drive performance up—and you're the one writing prompts, evals, and talking to users—you'll have a clear sense if it's meeting expectations. If prompt engineering runs out of steam, fine-tuning might be the next step. This isn't exclusive to designers, but they are often well-positioned to make that call.

A key design translation layer is remembering user assumptions. Engineers and designers working closely with models can forget that users don't know the intricacies, like reminding a model it's working in an HTML file for better output. Designers should engage their "inner doofus" and communicate what a naive user, unfamiliar with AI model quirks, might try and where they'd get stuck.

Hearing you speak reminds me of the importance of comfort with ambiguity. It's always been important, but now it feels even more important. Like, it used to be that there were infinite problems and you had to figure out the one experience that could solve that problem, but now our products are also infinitely dynamic experiences. It's not TurboTax where the user sees the same thing every time.

What other kinds of advice would you give to someone starting a new role where they're designing AI products for the first time?

The most sustainable and impactful thing is to invest significant time upfront to truly understand what goes into the model and what comes out. What's its prompt? What user information is fed in? What tools can it call? What evals are in place? Get an intuitive sense for what happens when you adjust these dials.

You don't want to be just the UI maker for an output you don't deeply understand. If engineers and PMs come to you saying, "The model gives you this, design an interface around it," you can do that, but you won't be able to propose meaningful improvements based on user insights. You'll also be working very reactively to subsequent model changes. You want to be part of the decision about whether a new capability is even something you want, not just on the receiving end.

Getting into that nitty-gritty can be challenging, especially for designers not code-literate. Your company might have interfaces like [Langsmith](https://www.langchain.com/langsmith), or you might need to learn to run the development environment yourself. It's a hard task, but crucial.

It kind of ties back to your advice to designers to review how the system works so that they can understand it. The time where you felt like you created the most value was when you were able to take a step back and, understanding the whole system, point out a simpler way to meet the users' needs.

Another example, though not a product per se, was the [LaMDA](https://blog.google/technology/ai/lamda/) launch (Google's early LLM). A lot of our time was spent just playing with the model, trying to prompt it—though we didn't call it "prompting" then—to get it to pretend to be different things and perform reliably. The demo we chose, where you could talk to Pluto or one of its moons, was purely a function of trying countless things and seeing what performed best. We couldn't have strategically picked that without extensive, hands-on experimentation to discover the model's strengths.

It seems like "Should designers prompt" is different in kind to "should designers code". Ultimately, with coding, the answers to those questions are pretty falsifiable - can we build XYZ with ABC technology? Asking an engineer the question is pretty equivalent to knowing the answer yourself. AI model behavior is inherently more subjective and nuanced. There's no substitution for understanding that material yourself at a deep level.

Barron, do you miss designing? From what we know of you, you've gone through doing evals, engineering. I'm curious in these last roles that you've had, do you still see this craft that you're taking on as a form of design? Do you still mock stuff up? Do you miss the craft part of aesthetics and structure?

It's a good question. I do think of it as design—it's just a very different form of it. You're designing behavior, and you may never get it perfect, which is fine. That's a different mindset from UI design, where you have full control over every pixel and perfection is rewarded.

I still mock things up and play with design tools. At Figma, I make eval cases, go through outputs, and fix what feels off. It's almost therapeutic—like a fidget spinner. Give me a website mockup and thirty minutes to fix the typography, and I'm happy. I still get doses of that kind of work, but it feels different now—and it's the kind of work that's never really done unless the feature gets removed, because you can always keep improving it.

## Hot Takes

One thing I've been thinking about—if I hadn't joined Figma, I was probably going to try to raise money for this. The idea I want someone to build, whether it's me or someone else, is this: the better all these AI tools get, the less information I can retain. I want a system that keeps my brain in shape, because you're offloading all these things to an AI. There are a lot of things you won't be practicing as frequently, or even doing manually. I definitely feel this way using Cursor. I get so much more done in a day than I would have two years ago. But if you ask me in a month how any of these algorithms work, I probably won't be able to answer you. Two years ago, it would have taken me a month to get done what I can do in a weekend now. But I would have learned it through pain, and retained it a lot more.

I don't really have a super strong opinion on that, actually, which is unfortunate. I feel like I've ceded thinking about that, because it's too -- if I start figuring out that, I'm like, Why do I even have a job? You know? There's certain stuff that I, like, deliberately ignore.

I think that it will look very different in five years than it does today, in the same way that I think that designers jobs, especially software designers jobs, have been less directly impacted by AI up until this point, in comparison to software development. If you think about the way that some software developers work now that are in the bleeding edge, they might have three different features they're working on all at the same time, and they're kicking off agents and reviewing their work, and cycling between them. That is not a product experience that exists for design right now. So, yeah, I think that it will look very different. I do hope that designers don't get left behind. A lot of the behavior of AI systems by default falls to the engineers, because they are the ones that have the tools to manipulate the system directly, they're the ones 

---

# [五条0109] a16z：AI工具的下半场是「探索」
发布日期：2026/01/08

![](https://mmbiz.qpic.cn/sz_mmbiz_gif/qpAK9iaV2O3sAVsSPfCN9UX44XiaoicbUJIrOGuaujdMNY6iaQewDZEX1GY3tcVk3QGeKJyUMMHBSMALvO8B7DZwsA/640?wx_fmt=gif&from=appmsg#imgIndex=0)

2026 年，基模会不会吃掉所有应用场景？

AI 应用层的「厚度」，是 AI 业内许多大佬都在探讨的一个关键问题。

Andrej Karpathy 在年度总结里有个观点：

> 基座大模型，就像一个刚毕业的大学生。LLM 实验室负责把它培养成一个什么都懂一点的通才。
> 
> 而 LLM 应用要做的，是把它招进来，然后进行「岗前培训」。用私有数据、传感器、执行器和反馈闭环，不断地组织、微调、打磨。
> 
> 最终，把这个「通才」毕业生，变成一个特定领域的顶尖专家。

近期，a16z 专注消费级应用的投资人Anish Acharya 围绕「2026 年，AI 应用生态可能是什么样的？」，也进一步聊了聊他认为的几个关键问题。

Acharya 提到，「AI 业内已经把代码的成本打下来了，但这种低成本的红利，还没有真正渗透到整个行业、乃至全世界。未来的公司形态、会出现什么样的新软件，我们对这些的理解，可能连 10% 都还没到。」

应用和模型的分化，会越来越明显。未来的 AI 应用会是一个组合体，把顶尖模型的调度能力、针对特定领域的 UI，以及现在已经便宜到不行的海量功能，全部打包在一起。

同时，Acharya 提出了一个关键的问题：最核心的底层的基础工具问题还没解决。现在所有的工具，都是重在执行，不是「思考」。简单来说，除了 LLMs 外，基本都是「动手做」的，但不是用来帮助用户「动脑想」的。

## **01** 

## 核心矛盾：思考工具 vs 执行工具

Acharya 预测，2026 年最大的变化，就是工具的本质。

我们现在用得，全是执行工具：

* IDE 用来写代码；
* Figma 用来画设计；
* Excel 用来算模型。

但如果想找个工具帮我们探索、思考，你会发现，除 LLM 可以当个聊天伙伴，市面上几乎没有这种产品。

**当 AI Agent 越来越强，核心难题就变了：从「我怎么把它做出来」变成了「我到底应该做什么」。**

你可以想象一个未来的产品经理（PM）：给 AI 定个大方向，每天早上起来，直接审阅 AI 在夜里自己头脑风暴、动手开发、顺便还做了 A/B 测试的两三个新功能。

Acharya 认为，现在的模型在「决定下一步做什么」这事上还很菜。想出来的点子，要么平平无奇，要么是别人的翻版，没有那种顶尖产品该有的「灵气」。

所以，**下一代编程、设计和生产力工具，核心战场将从「执行」转向「探索」。**

Cursor 等编程工具是这个路线下跑得最快的。Acharya 提到了谷歌近期推出的产品 Antigravity，以「agent first」打法，把探索放在第一位，非常有意思。

![](https://mmbiz.qpic.cn/sz_mmbiz_png/qpAK9iaV2O3ueG8mmruO4d9Ng0qg2WTQY0X4PlAMWHgGr7lEKUV0kSiaIyEzCuh8sd7wfAJaxMfpXWLzPGNPI9UQ/640?wx_fmt=png&from=appmsg#imgIndex=2)

> Antigravity 是一个「agent first」的 IDE，不再是写代码，而是作为一个指挥官，直接给 Agent 下达高级指令，然后自主地规划、编码、甚至打开浏览器去测试和验证自己的工作。

## **02**

## 未来，每个团队都得是个软件团队

软件公司里有两种部门：

* **权力部门**：工程、产品、营销。离软件最近，是发动机。
* **服务部门**：法务、财务、HR。离软件很远，更依赖人力。

Acharya 认为，AI 编程 Agent 的出现，会对企业带来两大冲击：

**第一，所有部门，都必须「软件优先」。**

从市场、法务、采购到财务，每个团队、每项任务，都必须「软件优先」。这些部门的领导，以后得先想想有什么软件工具能用，而不是像过去一样，先靠流程和招人来解决问题。

很多公司会用上像 Harvey 这种行业专用产品，也有的会直接用 Codex 或 Claude Code 这种「bare metal」类型的编程智能体。**未来，每个团队都得是个软件团队。**

**第二，公司的「野心」可以极大膨胀。**尤其是软件公司，对「应该做什么产品」的想象力，可以被彻底打开。整个创意和项目定优先级的流程，都得推倒重来。未来，只要是能做的功能，就一定会被做出来。大多数公司，还没准备好。

Acharya 的判断是：改变企业文化的难度，一点不比改变组织架构的难度小。

## **03**

## 应用和底层模型的分化，

## 会越来越明显

大模型推理进入第二年。很多人担心，应用层会被模型层取代。

Acharya 预测，应用和模型的分化，会越来越明显。

**未来的 AI 应用会是一个组合体，把顶尖模型的调度能力、针对特定领域的 UI，以及现在已经便宜到不行的海量功能，全部打包在一起。**

这也是 a16z 之前提到的「Narrow Startups」。

> 未来，想做大众市场的消费级创业公司，会越来越少。未来的赢家，是那些把产品做得更深、更窄的「Narrow Startups」。
> 
> 过去 15 年，消费软件就俩模式：要么免费看广告，要么一个月 20 美金。这就是天花板。所有人都觉得，用户不可能再多花钱了。
> 
> 但现在，人们乐于为 Claude、Grok 4 Heavy 和 Gemini 这些产品每月支付 200+美元。
> 
> 不是因为它们适合所有人，关键是能给特定用户带去 100 倍效率的提升。
> 
> 免费版只是个流量入口，是漏斗，200 刀的付费版，才是真正的产品。

极致的专业化，现在可行了。这也是为什么，应用不仅能独立于模型存在，而且还会走出完全不同的路。

顶尖实验室和科技巨头，跟他们做的模型一样，能力是「锯齿状」（jagged）的。在他们自己的核心领域，确实很牛。但各家也有自己的「包袱」，比如 Google 对监管的承诺，OpenAI 同时想做消费、企业、模型、硬件四个市场的「老大」。

**所以，「应用最后会被模型吃掉」，这个假设可能是错的。**

哪怕是在编程这种模型发展的核心领域，看到的也是一个很火的创业生态，光 2025 一年，新创造的收入就超过了 10 亿美金。

应用层的创业公司，有几条路优势很大：

* **多模态模型** **一定要用好**

大厂和实验室永远只推自家的模型。但在编程、创意这些领域，把各家最强的模型组合起来用，效果能直接拉满。

* **有独家数据**

很多赛道，创业公司已经锁死了独家数据集，效果比大厂强 10 倍，比如 Open Evidence 和 Vlex。

* **有网络效应和能复利循环的产品**
* **有功能极其丰富的生态系统**

用生态位的复杂性碾压单点功能。大厂抄一个 Granola 的录音功能当然容易，但也许不会费劲地去把功能背后一整套生产力 App 生态全做出来。

同时，结合 Karpathy 提出的 thick AI apps 应该有的几点特点：

> Karpathy 在年度总结中提到，Cursor 是一个全新应用层级的产品。Cursor 的火箭般增长，证明了在基础大模型之上，存在一个厚实且充满机遇的「应用层」。
> 
> Cursor 这样的 LLM 应用会针对特定垂直领域打包和编排 LLM 调用：
> 
> * 它们负责上下文工程（Context Engineering）；
> * 它们在底层编排多次 LLM 调用，串联成越来越复杂的有向无环图，在性能和成本之间精细权衡；
> * 它们为 Human in the Loop 提供针对特定应用的图形界面；
> * 它们提供一个自主性滑块，让用户决定 AI 可以自己做多少决定；

结合这两点，一个成熟的 AI 应用长什么样，基本就很清晰了。

## **04**

## 人类，终于开始发现 AI 的「另一面」

在探讨命令行界面（CLI）是怎么把普通消费者挡在 AI 的强大能力之外这个问题上，Eugenia Kuyda 一直是最具洞察力的思考者。

> Eugenia Kuyda：人人都知道聊天界面长什么样。但对普通人来说，聊天框的潜台词就只有两件事：聊天，最多再加个搜索。 模型真正的超能力，全都被藏在了普通人根本不会碰的「命令行」后面。

但现在，情况开始变了。Wabi（一款可以生成个人 app 的产品）是一个催化剂，它让普通人也能体验代码生成；ChatGPT 和 Grok 里的「图片」功能，对图像生成也起到了同样的作用；顺利的话，应用商店（Apps Directory）和 Skills 会让普通人也能玩转多模态内容生成（MCPs）和提示词插件。

Acharya 认为，让更多普通消费者自己动手用 AI 创造点东西，就能在一定程度上改变这个现状。2025 年，随手生成一个小应用的快乐，就跟 2023 年随手生成一首诗一样，但大多数人还不知道这件事。这也部分回应了 Nikita 提出的关于「谁在创造」的、一个颇为悲观的现实。

![](https://mmbiz.qpic.cn/sz_mmbiz_png/qpAK9iaV2O3ueG8mmruO4d9Ng0qg2WTQY6gVuhKdz35fOfKgRvtzu1RN2xxqQDwbZnHX47j2VlbuET9h7d22L4A/640?wx_fmt=png&from=appmsg#imgIndex=3)

## **05**

## 给 CEO 们的几点建议

对于那些已经掌管着规模化企业、并思考如何引领公司度过 AI 转型期的 CEO 们，Acharya 给出了几点想法。

**第一，去看标杆案例。**看他们是怎么用一个 AI，把销售、客服、催收所有面向客户的岗位，整合成一个部门的。

**第二，所有部门「软件优先」。** 让非技术部门也用上模型，是企业运营效率指数级提升的关键。

**第三，更大胆地做产品，也要敢于定更高的价格。**Tesla 的 FSD 已经能横穿美国了；Claude Code 已经能「自己写自己」了。 这意味着，对于绝大多数企业任务来说，短期意义上的 AGI 其实已经来了。

最后是，玩得开心点！没人会在好日子还在的时候告诉你，「你正身处黄金时代」。

这一轮产品周期，比近代任何一次都更去中心化、更软件驱动，对技术人来说，也有趣得多。

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3tgKBLnu3ESdbS3me6paicic7GtJVibuQJFwedF36nareXFGjUJdaqPC8e5sxqrsB9ok18Uq7Wtb5jVg/640?wx_fmt=jpeg#imgIndex=4)

![](https://mmbiz.qpic.cn/sz_mmbiz_jpg/qpAK9iaV2O3u2fI9s28mn09TnD4aChWibVHIyyBzPC2GibicVQ57QYiaEw6yibwy9zhkB7aFajGpNtBru6icEFuibRKXwA/640?wx_fmt=jpeg#imgIndex=5)

**更多阅读**

****[泛娱乐 AI 赛道观察： 从「猜你喜欢」到参与共创，角色才是 AI 时代最核心的资产](https://mp.weixin.qq.com/s?%5F%5Fbiz=Mzg5NTc0MjgwMw==&mid=2247522034&idx=1&sn=00d96c9d22128aed49d40f4642f64898&scene=21#wechat%5Fredirect)**

****[两次拿到陆奇投资，张浩然这次想用 Agencize AI 干掉所有工作流 Agent](https://mp.weixin.qq.com/s?%5F%5Fbiz=Mzg5NTc0MjgwMw==&mid=2247522023&idx=1&sn=cffe01b1d2107b5b20f9c9d7c6dc34e6&scene=21#wechat%5Fredirect)**

****[AI 陪伴赛道复盘：2026 年了，为什么还没有一款千万级 DAU 的产品跑出来？](https://mp.weixin.qq.com/s?%5F%5Fbiz=Mzg5NTc0MjgwMw==&mid=2247521968&idx=1&sn=0b287d4056c7844de5791232d0f3251b&scene=21#wechat%5Fredirect)**

****[想成为下一个 Manus，先把这些出海合规问题处理好](https://mp.weixin.qq.com/s?%5F%5Fbiz=Mzg5NTc0MjgwMw==&mid=2247521960&idx=1&sn=7bbb8c8b7f2bb2ce42b45c665d48dccb&scene=21#wechat%5Fredirect)**

![](https://imgproxy.readwise.io/?url=http%3A//mmbiz.qpic.cn/mmbiz_png/qpAK9iaV2O3sJgoBraTTtb4HvpIUu0gjF1iaeu47FWB2mazlo7BSDYJJYk3Td5HVich4ag1IUbv1T7208rVBVqg5w/300%3Fwx_fmt%3Dpng%26wxfrom%3D19&hash=508cda7c17f59cfb67d6e489f4146219)

**Founder Park**

来自极客公园，专注与科技创业者聊「真问题」。

481篇原创内容

公众号

转载原创文章请添加微信：founderparker

hidden text to trigger resize events if fonts change

---

# [五条0109] 谷歌AI逆袭：从翻车到反超OpenAI
发布日期：2026/01/08

华尔街日报最新发了一篇长文《How Google Got Its Groove Back and Edged Ahead of OpenAI》，讲的是 Google 在 AI 竞赛中如何从落后到反超的故事。

2025 年 8 月的一个凌晨两点半，Google 的一位 AI 项目经理 Naina Raisinghani 正在上传 DeepMind 实验室的最新成果，一个超快的图像生成器。她需要给它取个名字才能提交到 LM Arena 排名平台，但这个点没人在线。

于是她随手用朋友给她起的两个外号拼了一个：Nano Banana🍌。

几天后，Nano Banana 冲上排名榜首，在 X 上成为热门话题，用量远超 Google 预期。到 9 月，Gemini App 成了苹果应用商店下载量第一。11 月，Google 发布了迄今最强的 Gemini 模型，在多项指标上超越 ChatGPT，股价大涨。

OpenAI 内部随即发出 Code Red。

时间拨回 2015 年，Pichai 刚接任 Google CEO，那时候 AI 还只是学术圈的事。2016 年他发了一篇博客，说过去十年是智能手机的时代，未来十年将是 AI 优先的时代。

其实 Google 早就在布局。2011 年成立了 Google Brain，联合创始人 Jeff Dean 参与开发了神经网络技术，这是今天大语言模型的基础。后来又收购了伦敦的 DeepMind，创始人 Demis Hassabis 是个国际象棋神童，后来因为 AlphaFold 拿了诺贝尔奖。

还有一步棋当时不太引人注目：Google 开始自研 AI 芯片。他们认为语音识别这类应用会需要大量算力，于是设计了 TPU（张量处理单元），比传统 CPU 和 GPU 更省电。这步棋后来被证明是关键。

不过 Google 一开始对聊天机器人很谨慎。一些高管和研究员担心安全问题，早期模型很容易被诱导出种族歧视或性别歧视的回答。前 Google Brain 员工 Julia Winn 说，Google 对这类风险看得比她待过的任何公司都重。

这种谨慎让一些研究员很沮丧，有的选择了离开。

2022 年 8 月，Google 发布了一个叫 LaMDA 的聊天模型，只开放给少数人测试。测试 App 叫 AI Test Kitchen，有三个功能：想象它、列出它、聊狗。对，第三个功能只能聊狗。

三个月后，OpenAI 发布了 ChatGPT。五天内，一百万人注册。用户没有太多限制，想聊什么聊什么。

Google 内部一些在 AI 上耕耘多年的员工气坏了。分析师和投资者开始质疑：Google 是不是要错过科技史上的下一波大浪？

2023 年 1 月，Jeff Dean、Demis Hassabis 和新加入的机器人专家 James Manyika 向董事会汇报了打造最强模型的计划。

但 Google 等不及了，需要先推一个产品出来。2 月，他们匆忙发布了基于 LaMDA 的 Bard。

发布会翻车了。宣传视频里，Bard 被问到韦伯望远镜的问题，回答说它拍了第一张系外行星照片。这是错的。Alphabet 股价当天跌了 8%。

差不多同一时间，已经退休的联合创始人 Sergey Brin 在一个派对上碰到了 OpenAI 的研究员 Daniel Selsam。Selsam 问他：ChatGPT 这么厉害，作为计算机科学家你不心动吗？怎么不回来全职搞 AI？

Brin 觉得他说得有道理，于是回归了。

2023 年大部分时间，Google 都在努力整合内部的 AI 力量。Google Brain 偏研究，DeepMind 偏产品，两边文化不同，合并后产生了不少摩擦。

不过 Google 有一个巨大优势：OpenAI 需要融资，Google 可以从自己几百亿的利润里拿钱做研发。

但 Google 还有一个难题：怎么在拥抱生成式 AI 的同时，不把自己的摇钱树给弄死？Google 占了网页搜索市场 90% 的份额，这是广告业务的根基。

为了弄清楚 AI 搜索应该长什么样，Google 启动了一个叫 Project Magi 的多团队项目，由后来成为搜索副总裁的 Liz Reid 牵头。

Reid 说，难点在于当答案不在单个网页上时，怎么让搜索快速给出清晰的回答。人们不只是在用搜索，而是在依赖搜索。搞砸了的话，你妈、你朋友、你孩子都会来找你算账。

2023 年底，Google 发布了第一版 Gemini。OpenAI 的 ChatGPT 主要用文本训练，Google 的 Gemini 从一开始就用文本、代码、音频、图像和视频一起训练。这是技术野心更大的方案，虽然开发时间更长，但后来证明是值得的。

2024 年 5 月，Google 推出了 AI Overviews，在搜索结果顶部显示 AI 生成的摘要。用户开始进行更复杂的搜索。随后 Google 开发了 AI Mode，一种聊天机器人式的搜索选项。

Reid 说，经过无数次迭代，团队开始发现自己不再只是为了测试而用它，而是真的想用它。

Brin 回来后做的很多工作是帮 Gemini 挑毛病。他还促成了一笔 27 亿美元的收购，把两位离开 Google 创业的 AI 研究员 Daniel De Freitas 和 Noam Shazeer 带了回来。这两人后来参与领导 Gemini 的开发。

2024 年 8 月，Nano Banana 爆火。负责 Gemini App 和 Google Labs 的 Josh Woodward 把这次发布称为成功的灾难：全球用户生成了数十亿张图片，Google 一度找不到足够的算力，只能紧急借用服务器。

到 10 月，Gemini 月活用户从 7 月的 4.5 亿涨到了 6.5 亿。

11 月 Gemini 3 发布又造成算力瓶颈。但 Google 十多年前就在准备这一天了。他们自研的 AI 芯片成了竞争优势，最新的 Ironwood 芯片大幅降低了 AI 模型的运行成本。

11 月底传出消息，Google 正在和 Meta 谈判，要卖给他们价值数十亿美元的芯片。这个消息让 Nvidia 股价当天跌了 7%。

Pichai 在 12 月的内部备忘录里写道：我们以很棒的姿态结束了 2025 年。想想一年前我们在什么位置，这个进步令人难以置信。

Google 用了十多年的积累，经历了 ChatGPT 的冲击和 Bard 的翻车，整合了内部资源，最终在 2025 年底完成逆袭。当然，OpenAI 后来也发布了更强的 ChatGPT，用户量仍然远超 Gemini。这场 AI 竞赛远没有结束。

<https://t.co/W3yXCBmv8g>  

![](https://pbs.twimg.com/media/G-KbtuOXMAAZ88a.jpg)

hidden text to trigger resize events if fonts change

---

# [五条0109] Claude Skills：让AI成为你的专属打工人
发布日期：2026/01/09

你有没有这种感觉 ——AI 领域的新名词，比手机型号更新还快？

昨天刚搞懂「函数调用」(Function Calling) 是怎么回事，今天又冒出来个「Skills」。前天有人跟你说「MCP」，你还没反应过来，后天又有人聊「Agent」。每次看到这些词，第一反应都是：我是不是又落伍了？

别慌。今天咱们把「Claude Skills」这事儿掰开揉碎了讲清楚。

更重要的是，我会告诉你它跟你已经知道的那些概念 —— 函数、函数调用 —— 到底是什么关系。你会发现，这不是三个孤立的新词，而是一层一层往上搭的台阶。搞懂这三层，以后再出什么新词，你也能自己判断它在哪一层。

## 起点

  
咱们从最熟悉的东西开始：编程里的「函数」。

你可以把函数理解成一个「小助手」。你告诉它要做什么（给它一个输入），它帮你做完后告诉你结果（给你一个输出）。就像餐厅里的服务员：你点菜，他端菜，每次都按照固定流程来。

举个例子，程序员写一个叫 `calculate_tax (income)` 的函数。你把收入数字扔进去，它把该交多少税算出来还给你。下次还要算？再调用一遍就行。不用每次都重写一遍算税的逻辑。

函数的价值，说白了就三个字：**封装、重用、标准化**。

把一件事情的做法「打包」起来，以后谁都能用，每次用的方式都一样。这是程序员几十年来最基本的生产力工具。

![](https://pbs.twimg.com/media/G-L7CmQW0AAkUVK.jpg)

但函数有个局限 —— 它只活在代码世界里。

程序员在代码里写 `getWeather()`，这个函数 100% 会被执行。可是普通人不会写代码，AI 也不会直接「运行」这些代码。那怎么让 AI 也能用上这些「小助手」呢？

## 架桥

  
2023 年前后，一个叫「函数调用」（Function Calling）的概念火了起来。

你可以把它理解成：给那个「只会聊天的 AI」配了一部电话和一本通讯录。

以前的 AI，你问它「今天北京天气怎么样」，它要么从训练数据里瞎猜一个，要么老老实实说「我不知道」。因为它没有「手脚」，不能真的去查天气。

有了函数调用之后，情况变了。

开发者事先告诉 AI：「这是一本通讯录，里面有个叫 `get_weather` 的函数，你想查天气就打这个电话。」AI 收到「今天北京天气怎么样」的问题后，它会自己判断：「哦，这个问题我得打电话给 `get_weather` 才能回答。」

然后它生成一段标准格式的「便签」（叫 JSON），上面写着：

{

 "function": "get\_weather",

 "arguments": {

"city": "北京"

 }

}

这段便签被外部程序接收、解析、执行。真正打电话给气象台的，是外部程序，不是 AI 自己。执行完后，结果返回给 AI，AI 再用人话告诉你：「北京今天晴，15 度。」

**这里有个关键的转折，初学者容易忽略。**

传统函数是「确定性」的——程序员在代码里写了 `getWeather()`，100% 会执行。

但 LLM 的函数调用是「概率性」的——AI 看到「今天天气怎么样」，它要**自己判断**该不该调用天气函数。这个判断是基于理解，不是基于规则。有小概率它会判断错误，比如把「天气」理解成某个人名。

所以说，函数调用的本质是：**让 AI 能「打电话」，但打不打、打给谁，它自己决定。**

这是一个巨大的进步 ——AI 不再只是「知识库」，它开始变成「行动者」。

![](https://pbs.twimg.com/media/G-L60Z9XYAAy8sV.jpg)

但函数调用还有个问题：它是零散的、一次性的。

你给 AI 配了十几个函数，它每次只能选一个打电话。如果一个任务需要连续调用五六个函数、中间还有逻辑判断、还需要参考一些文档，函数调用就不够用了。

## 跃升

  
2025 年 10 月 16 日，Anthropic 发布了一个新功能：[Claude Skills](https://support.anthropic.com/en/articles/12512176-what-are-skills)。

你可以把 Skills 理解成「员工手册」+「工具箱」的组合。

员工手册告诉 AI：「当你遇到某类任务时，应该怎么做，分几步，每一步用什么工具。」工具箱里装着它需要用的脚本和参考资料。

具体来说，一个 Skill 就是一个文件夹，里面有三样东西：

**第一，[SKILL.md](http://SKILL.md) 文件**。这是「指令」，用自然语言写的。告诉 AI：这个 Skill 是干什么的，什么情况下该用，怎么用，有什么注意事项。

**第二，脚本**。可以是 Python、JavaScript 或者其他语言写的代码。当 AI 需要「动手」的时候，就执行这些脚本。

**第三，资源文件**。比如参考文档、模板、配置文件。AI 在执行任务的时候可以查阅这些资料。

你可能会问：这跟函数调用有什么本质区别？

区别在于：**函数调用是「单个工具」，Skills 是「整套解决方案」​**。

打个比方。函数调用像是给你一把锤子、一把螺丝刀、一把扳手，你得自己知道什么时候用哪个。Skills 像是给你一本《如何组装宜家书柜》的说明书，说明书里不仅告诉你步骤，还附上了所有需要的工具和零件。

还有一个重要的机制叫「渐进式披露」。

AI 的「工作记忆」是有限的（技术上叫「上下文窗口」）。如果你把所有 Skills 的内容一股脑塞进去，AI 会被信息淹没。

Skills 的做法是：平时只告诉 AI「有这么一本说明书」，AI 真正需要的时候再去翻。就像你不用把整本百科全书背下来，遇到问题再查相关那一页就行。

![](https://pbs.twimg.com/media/G-L6bveXMAA3vgQ.jpg)

现在，咱们把三层放在一起看：

![](https://pbs.twimg.com/media/G-L6QRTW0AAtfzD.jpg)

从下往上看，抽象层次越来越高。函数是代码级的，函数调用是接口级的，Skills 是工作流级的。

**Skills 可以包含函数调用，但函数调用只是 Skills 的一部分。**

就像一本菜谱不只是「切菜」「炒菜」「装盘」这几个动作的罗列，它还包括「为什么要这样做」「火候怎么掌握」「如果烧焦了怎么补救」这些知识。

## 实战

  
说了这么多概念，Skills 到底能干什么？咱们看几个真实案例。

**先说一个我自己的项目：[x-article-publisher-skill](https://github.com/wshuyi/x-article-publisher-skill)**。

![](https://pbs.twimg.com/media/G-L6FxcWcAAZufc.jpg)

如果你用 Markdown 写文章，然后想发布到 X（Twitter）的 Articles 功能里，你会遇到一个极其崩溃的问题：复制粘贴过去，格式全丢了。

标题变普通文字，粗体变普通文字，链接变普通文字。你得手动一个一个加回来。一篇文章，光是调格式就要花 15-20 分钟。

更要命的是图片。你得手动上传每一张，然后把它拖到正确的位置。如果文章有十几张图，你很容易搞错顺序。

这个 Skill 怎么解决的呢？

它会先解析你的 Markdown 文件，提取标题、封面图，并且给每张内容图片计算一个「块索引」（block\_index）—— 就是这张图应该出现在文章的第几个段落之后。

然后，它把 Markdown 转成富文本 HTML，通过剪贴板粘贴到 X 编辑器里。格式完美保留。

最后，它用浏览器自动化（Playwright）把每张图片精准插入到正确的位置。

原本 20-30 分钟的手动操作，现在几分钟内**全自动搞定strong>。减少时长自不必说。对懒人而言，能全**程不用自己再动手，才是最主要的嘛。

你可能会说：这不就是写了个自动化脚本吗？

是，也不是。

单纯的自动化脚本，你得自己记住什么时候用、怎么用、参数怎么填。但 Skill 把「什么时候用」「怎么用」都写进了指令里。你只需要跟 AI 说一句「把这篇文章发到 X」，它就知道该调用这个 Skill，该怎么操作。

**这就是「知识编码」的价值——把「我知道怎么做」变成「AI 也知道怎么做」。**

再看几个企业场景。

**会议管理**：有个 Skill 能自动从会议记录里提取摘要、决策和行动项，然后起草跟进邮件。开完会不用再花半小时整理笔记。

**数据分析**：扔给它一个 CSV 文件，它能自动识别关键指标、找出异常值，生成图文并茂的报告。非技术人员也能快速从数据里挖出洞察。

**客户支持**：它从公司知识库里检索准确答案，再用人性化的语言组织成回复。既保证准确，又不失温度。

这些场景有个共同点：**都是重复性高、步骤固定、但又需要一定判断力的任务**。以前，要么靠人硬扛，要么花大价钱开发专门的软件。现在，一个 Skill 就能搞定。

**最后说说开发者工具。**

有个叫 `skill-creator` 的 Skill，特别有意思 —— 它是用来创建 Skill 的 Skill。

![](https://pbs.twimg.com/media/G-L5tI9XMAAALwG.jpg)

你跟它聊天，告诉它你想实现什么工作流，它会帮你生成一个完整的 Skill 项目框架。这就是所谓的「元技能」。

还有 `webapp-testing`，能根据测试用例自动操作浏览器，对 Web 应用进行功能测试，然后生成测试报告。前端测试流程部分自动化了。

## 动手

  
说了这么多，怎么开始用 Skills？

**如果你想用现成的 Skills**，最简单的方式是通过 Claude Code 的插件市场。

默认初始自动安装的只是 Claude 官方的插件市场。

![](https://pbs.twimg.com/media/G-L5iWgW8AAiyG6.jpg)

你也可以根据自己的需求，添加其他插件市场。格式例如：

/plugin marketplace add anthropics/claude-code

![](https://pbs.twimg.com/media/G-L5XpNXQAAw35c.jpg)

安装之后，你这里就有两个插件市场了。

![](https://pbs.twimg.com/media/G-L5M0jXAAApqJv.jpg)

你看到了，利用 `/plugin` 命令，可以添加、管理插件。

这是我目前已经安装的部分插件。

![](https://pbs.twimg.com/media/G-L43HPW8AAUt3e.jpg)

装完之后，你可以让 Claude 用某个 Skill 来完成任务。比如：「用 PDF Skill 提取这份文件里的表格数据。」

**如果你想自己创建 Skills**，可以用 `skill-creator` 这个元技能。跟它对话，描述你的工作流，它会帮你生成框架。

你可以编写 Claude Skill，让它帮助你去分析材料、自动调研，并且绘制出对应的结构图。

例如这是红楼梦人物关系图。

![](https://pbs.twimg.com/media/G-L4pI3WUAACQ6n.jpg)

下面是战国七雄的互动。

![](https://pbs.twimg.com/media/G-L4eR-WwAASYi-.jpg)

使用方式可以 [参考我这篇文章](https://mp.weixin.qq.com/s/edyLjMcarzIrjRi3vsuCPQ)。

更高级的玩法是用 Claude Skills 连接一些非常好的外部工具，例如 NotebookLM 作为知识库使用。这样你就可以把 NotebookLM 强大的检索和知识验证功能，与你自己的创意以及其他模型工具的特点有机地结合在一起。

![](https://pbs.twimg.com/media/G-L3sgkWMAAJGwr.jpg)

我在 [这篇文章当中有详细的介绍。](https://mp.weixin.qq.com/s/lrAeILr8qAJjrMXmnK339g)

想看看别人做了什么 Skills？去 GitHub 搜 [awesome-claude-skills](https://github.com/travisvn/awesome-claude-skills)，那里有社区整理的优秀 Skill 清单。

![](https://pbs.twimg.com/media/G-L3lr2WEAAyOmu.jpg)

我个人比较推荐的是活水智能（也就是阳志平老师团队）做的 [插件市场 42plugin](https://42plugin.com/) 。

![](https://pbs.twimg.com/media/G-L3eOyXQAAMxPT.jpg)

这里不仅整理了很多的插件，而且还有相应的评级评分，更可以保证避免踩坑。

![](https://pbs.twimg.com/media/G-L3Vn0XcAAfUAb.jpg)

最重要的一点：**创建 Skill 不一定需要会写代码。**

[SKILL.md](http://SKILL.md) 里的指令是自然语言写的。如果你的工作流不涉及复杂的脚本，光靠自然语言指令就能完成很多事情。

正如 Claire Vo 在 [Lenny's Newsletter](https://www.lennysnewsletter.com/p/claude-skills-explained) 里说的：即使是非程序员，也可以通过清晰地定义工作流，创建出强大的、可复用的 AI 工作流。

![](https://pbs.twimg.com/media/G-L3NNRWAAAYaoz.jpg)

## 小结

  
现在咱们回头看这三层台阶：

\- **编程函数**是基石。它提供了最基础、最可靠的逻辑执行单元。

\- **LLM 函数调用**是桥梁。它让 AI 不再只是「知识库」，而是能「打电话」驱动外部世界的行动者。

\- **Claude Skills**是蓝图。它把零散的工具和指令整合成完整的工作流，让 AI 能更可靠、更专业地完成复杂任务。

这三层会越来越融合。开发者继续写高效的函数作为底层工具；通过函数调用把工具暴露给 AI；再用 Skills 指导 AI 怎么智能地使用这些工具。

**真正的力量在于：它让「领域专家」也能「教」AI。**

你不需要是程序员，只需要清楚自己的工作流程是什么，就能把这些知识打包成一个 Skill。你的专业知识不再只存在于你脑子里，它变成了 AI 可以调用的能力。

对了，就在我写这篇文章的时候（2026 年 1 月 8 日），Claude Code 又发布了一次重大更新。Skills 现在支持隔离上下文、热重载、指定模型、在子代理中使用……[Plugin Marketplace](https://code.claude.com/docs/en/discover-plugins) 也正式上线了。

![](https://pbs.twimg.com/media/G-L3FyFXUAAmUBV.jpg)

Anthropic 还把 Agent Skills 规范作为 [开放标准](https://aibusiness.com/foundation-models/anthropic-launches-skills-open-standard-claude) 发布 —— 这跟他们之前推 MCP（Model Context Protocol）的路数一样，都是走开放生态路线。

Gartner 的分析师说，这标志着 AI 市场的焦点正在从「模型更新」转向「用例落地」。

翻译成人话就是：大家不再只比谁的模型更聪明，而是开始比谁能把 AI 用得更好。

Skills 是这场转变的核心载体。它让 AI 从「应答者」变成了「协作者」。

下次再有人跟你说什么关于 Agent 功能的新词，你就问自己：它是在哪一层？是代码级的工具，是接口级的桥梁，还是工作流级的蓝图？

想清楚这个，新词就不再可怕了。

你有没有尝试过 Claude Skills？有没有自己制作符合你自己工作流的 Claude Skill？

欢迎分享在留言区，咱们一起交流讨论。

## 延伸阅读
  
  
* [品味还是技能？ChatGPT 引发的能力培养变革](https://mp.weixin.qq.com/s/cSysHNhu24H8dUpKk9XxQg)
  
* [AI 时代的真稀缺技能：从「有技术」到「会洞察」](https://mp.weixin.qq.com/s/xEAWepdqRlq2kG6-rEtzbQ)
  
* [如何用 Claude Skill 帮你一句话做深度调研并自动画图？](https://mp.weixin.qq.com/s/edyLjMcarzIrjRi3vsuCPQ)
  
* [从枯燥理论到生动实践：AI 智能代理如何用交互式教程讲解复杂概念](https://mp.weixin.qq.com/s/9o2xEsAClOnrn2GhuBryBA)
  
* [新学期，给你自己配一个好用的 AI 助手吧。会思考，能联网，还有知识库那种](https://mp.weixin.qq.com/s/1Ce7TKTIju8GEHMbSmUIBA)
  
hidden text to trigger resize events if fonts change